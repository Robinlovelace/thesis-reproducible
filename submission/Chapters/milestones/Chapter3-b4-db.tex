% Chapter 3

\chapter{Spatial microsimulation and its application to transport problems}
\label{Chapter3}
\lhead{Chapter 3. \emph{Spatial microsimulation and % Definitely worth writing
% as a stand-alone paper
its application to transport problems}} % Write in your own chapter title to set

\begin{quote}
The modellers' task is to predict how people and organisations will live in
`good' and `sustainable' cities; how the infrastructure will, or should, grow;
and how activities and traffic flows are, where appropriate, best managed,
priced and regulated.
\flushright{\citep[p.~3]{Wilson1998-past}}
\end{quote}

Microsimulation can have variable meanings depending on whether you are a
geographer, transport planner, or economist
(see \citealp{Ballas2005b, Liu2006, Bourguignon2006} for examples).
This chapter introduces static spatial microsimulation, a particular type of
microsimulation that is is central to the PhD. The method enables
individual-level and geographical variation in commuting behaviour and hence
energy use to be analysed in tandem. Operational definitions, based on
established research, are important for clarity, repeatability, and to
show how the PhD builds on past research. A number of key terms will be
frequently used in throughout the thesis, so we begin this chapter with
definitions.
This is followed by an overview of the history (\cref{s:history}) and current
state of the art (\cref{s:sotart}) of the technique as it relates to transport
problems such as travel to work.

As stated in the quotation above, transport does not happen in isolation from
other phenomena. It is part of the complex web of the environment,
infrastructures, economics, policies, decisions and
resulting behaviours that define modern settlements. From this
perspective, spatial microsimulation for transport applications is just one
branch of a long-standing tradition of `urban modelling'
\citep{batty1976urban}.
Other branches include dedicated transport modelling techniques
(e.g.~\citealp{SATURN2012}), integrated land-use transport  models
\citep{Wegener2009} and agent-based simulations \citep{Gilbert2008-abm}.
These are
closely related to the methods presented in this thesis, and have
the potential to build on its results. These related research areas are
discussed in \cref{s:urbanmodel}. The final section of
this chapter (\cref{s:bigdata-gps}) considers new research directions that have
been made possible by  the availability of `big data' harvested from the
internet or alternative sources such as GPS loans to survey participants.
New approaches are already
making substantial contributions to understandings of personal travel.
Harnessed correctly, these new data sources have the potential to improve the
energy use estimates resulting from microsimulation models and shed insight into
the validity of assumptions made, as discussed in \cref{s:applications}.

\section{Definitions}
\label{s:defs}
\emph{Microsimulation}, as its name suggests, refers to the modelling of
individual units --- e.g.~people, household, companies --- which operate in a
wider system. Used in this sense, the term originates in economics, where it
signified a theoretical turn away from aggregate-level analyse and towards a
focus on individual behaviour. ``This shift of focus, from sectors of the
economy to the individual decision making units is the basis of all
microsimulation work that has followed from Orcutt's work''
(\citealp[p.~145]{Holm1987}; see \cref{s:digirev} for further reference to
this work). Microsimulation overall therefore has a wide meaning, from
individual vehicles in a transport model \citep{Liu2006, Ferguson2012}
to the inventories of
competing firms over time \citep{Bergmann1990a}.
The term has a narrower definition in this
thesis, however, that is more concerned with modelling the distribution of
behaviours of individuals over space than over time. This PhD is predominantly
concerned with only one subset of microsimulation: spatial
microsimulation, modelling the distribution of individuals over
space. Within the category of \emph{spatial} microsimulation we can
we can specify further (\cref{types-msim}).

\emph{Spatial microsimulation} of the static kind can be formally
defined as
follows: the simulation of individual-level variables within the geographic
zones under investigation \citep{ballas2003microsimulation-30-years, Ballas2007simb}.
The models that perform this operation have also
been referred to as `population synthesizers' \citep{Mohammadian2010}. This
term is useful in the context of transport applications, because small area
micro-population generation is only one stage of a wider process of
individual-level transport modelling (\citealp{Pritchard2012}; \cref{f:msim-schematic}).

\begin{figure}[h]
 \centering
 \includegraphics[width=16 cm]{msim-schematic}
 % circuity.png: 550x450 pixel, 72dpi, 19.40x15.88 cm, bb=
 \caption[Schematic a transport simulation model]{Schematic
of the components of a complete transport simulation model such as
TRANSIMS, after \citet{nagel1999transims} and \citet{Mohammadian2010}.
This thesis is primarily concerned with the first two stages.}
 \label{f:msim-schematic}
\end{figure}

During static spatial microsimulation individuals are sampled from a non-geographical
dataset via  reweighting, based on `constraint variables' that are present in
both individual-level and geographically-aggregated data
sets.\footnote{These
constraint variables are categorical variables (such as
`male', `age: 16 to 19' or `works 0 to 2 km away from home')
that are shared between the micro-level data and
known geographical aggregates, usually from the census.
}
% The next 2 paras are taken from ints3.tex
\Cref{f:msim-schematic} shows the technique in the wider context of
transport modelling. Spatial microsimulation
here refers to only the top two stages in the diagram. It represents a
computationally small, but important (for social analysis at least)
part of the wider simulation process. It is important to clarify this
distinction, as the meaning of `spatial microsimulation' can be ambiguous.
It can refer
either to the process of population synthesis
\citep{chin2006regional, Ballas2005c, Hynes2008},
or the entire urban modelling process that
builds on the spatial microdata \citep{Wegener2011}.
Spatial microsimulation here refers to the former case, and the results could
thus be harnessed as inputs into more complex dynamic models in which
each individual interacts with each other and other entities in a wider urban model.
The terms \emph{dynamic spatial microsimulation} or \emph{agent-based models}
will be used to refer to the wider modelling process.

Static spatial microsimulation (generally and henceforth referred to simply as
spatial microsimulation) involves sampling rows of
survey data (one row per individual, household, or company) to generate lists of
individuals (or weights) for geographic zones that expand the survey to the
population of each geographic zone considered. The
problem that it overcomes is that most publicly available
census datasets are aggregated, whereas individual-level data are generally
much more detailed \citep{ballas2003microsimulation-30-years}.
The ecological fallacy \citep{Openshaw1983}, for example, can be tackled
to some extent using individual-level data allocated to geographical zones
\citep{Hermes2012a}. This `spatial' or `small area' microdata is the output
of spatial microsimultion.

Spatial microsimulation cannot replace the `gold standard' of real,
small area microdata \citep[p.~4]{Martin2002}. From the perspective of social
scientists, it would be preferable for governments around the world to follow
Sweden's example and release such small area microdata anonymously. This
prospect is unlikely to materialise in the UK, adding importance to the process
of model validation. In any case, the experience of spatial microsimulation
development and testing would help prepare researchers for the analysis of real
spatial microdata. The method's practical usefulness (see \citealp{Tomintz2008})
and testability \citep{Edwards2009} are beyond doubt.

Assuming that the survey microdataset is representative of the
individuals living in the zones under investigation,\footnote{The suitability
of this assumption is further discussed in \cref{Chapter8}.} the
challenge can be reduced to that of optimising the fit between
the aggregated results of simulated
spatial microdata and aggregated census variables such as age
and sex \citep{Williamson1998}. These variables are often
referred to as `constraint variables' or `small area constraints'
\citep{Hermes2012a}. The term `linking variables' can also be used, as they
\emph{link} aggregate and survey data.
Based on the literature, technique seems to have been used for five main
purposes, to:
\begin{itemize}
 \item model variables whose spatial distribution at the aggregate-level is
otherwise unknown (e.g.~\citealp{Ballas1999}),
\item estimate the individual-level distributions of variables within small
areas about which only aggregate counts or summary statistics are know (e.g.
distance travelled to work)
\item understand the spatial distribution of discrete behaviours
(such as visiting `stop smoking' centres --- \citealp{Tomintz2008})
and thus the likely local-level effects of policy change \citep{Ballas2001}, 
\item project future changes at the local based on past trends
\citep{Ballas2005}, and
\item provide a foundation for agent-based models, which rely on
discrete individuals \citep{Ballas2007simb, Pritchard2012}.
\end{itemize}
The main purposes of spatial microsimulation in this PhD are bullet points one
and two above, although elements of every purpose will be harnessed at some
point.

In essence, spatial microsimulation merges individual level data (a list
of individuals, each with their own ID) with geographical data (a list of
zones, each with its own ID). It therefore relies on two types of input data:

The \emph{microdataset} is the individual level data from which individuals are
weighted or probabilistically selected from. It is referred to as the survey
dataset \citep{Wu2008} or simply as `individual data' \citep{Simpson2005}.
The input microdata should be as representative of the zones being studied as
possible\footnote{For
example, the date of survey data collection should be
close to date of at which the zonal data was collected. Also, the survey data
should preferably be from the same geographic region as the zones under
investigation, or at least weighted so that individuals from the region under
investigation are more likely to be sampled \citep{Ballas2005a}. An alternative
way of making the survey dataset more representative is to preferentially
sample individuals from areas with the same classification as the 
their zone being modelled.
}
and sufficiently diverse. %%%!!! More here!

The \emph{constraint variables}, `small area constraints' or `linking
variables' are the aggregate-level variables that link the zonal and individual
datasets together. The must be categorical, and the categories in the two
datasets must match up.

\emph{Reweighting} is the process by which individuals are assigned a weight
for each of the zones under investigation. The higher the weight for a
particular area, the more representative is the individual of that area,
compared with the rest of the survey dataset. Combinational optimisation and
deterministic reweighting are the two main methods for reweighting.

\emph{Combinatorial optimisation} \index{combinatorial optimisation} is an
approach to reweighting that uses repeated randomised sampling to
repeatedly select individuals from the survey microdataset and allocate them to
zones. Based on the fit between simulated and known aggregate counts after each
iteration, the parameters of the resampling algorithm can be adjusted (e.g.~in
simulated annealing).

\emph{Deterministic reweighting} refers to non-random methods of allocating
weights to individual-zone combinations. Iterative proportional fitting (IPF)
is a widely used deterministic reweighting algorithm, and is used in the
spatial microsimulation models presented in this PhD. If whole cases are
needed, integerisation can be be applied to the non-integer weight matrix.

\emph{Integerisation} is the process by which integer weights are generated
from the non-integer weight matrix. 

\emph{Cloned individuals} \index{cloning} are rows in the survey microdataset
that have been replicated more than once in the spatial microdataset for a
particular area. The cloning of individuals can be represented by an integer
weight above one, or simply by repeating identical rows multiple times. In
practice these two forms of representing data are interchangeable; the latter
takes up more disk space.


\section{The history of spatial microsimulation}
\label{s:history}
This section outlines the history of spatial microsimulation. It would
be easy to repeat past work here.\footnote{Readers interested in a
comprehensive history of the field are directed towards
\citet{ballas2003microsimulation-30-years}.} To avoid this, we focus on
developments that affect the way spatial microsimulation
is and can be used for transport applications. These include:
\begin{itemize}
 \item Consideration of the influence of location on individual behaviour via
 transport costs 
 \item The question of data vs theory-driven approaches
 \item Converting a spatial microdataset into a behavioural model
 \item The impact of rapidly advancing computers and data sources
\end{itemize}
These themes are present throughout the section, which is ordered
roughly chronologically.

\subsection{Pre-computer origins}
The theoretical origins of spatial microsimulation stretch back to before the
turn of the 20$^{th}$ century. It was only with the emergence of large scale
data sets, methods of analysis and and conventions of mathematical notation that
quantitative analysis of variables that vary over time and space could actually
occur \citep{ballas2003microsimulation-30-years}.
Despite (or perhaps partly because of) the absence of these pre-requisites for
the analysis and simulation of large populations at the individual level, much
progress was made in thinking about how individuals behave within environments
that vary in predictable ways over space before computers were invented.
Consideration of travel costs (which were much higher before most
people travelled by motorised modes) was integral to both Christaller's
central place theory, and Von Th\"{u}nen's concentric agricultural zones.
Lacking reliable data with which to test their ideas, the
early quantitative geographers had to make-do by
developing theories based on personal observation.
Some of these theories are still influential today \citep{Clarke1985}.
Ideas developed in the pre-computer age can be seen as the theoretical
forefathers of the microsimulation models of transport behaviour, and frameworks
for interpreting the results, that are in use today.

One explanation for the greater theoretical focus of pre-computer work
is that empirical data seldom fits into any neat model, and therefore
distracts from explanation.
This point was made as early as the 1970's, accompanied by the
warning that the
accelerating deluge of new datasets and quantitative methods was leading some
to conflate quantification with theory \citep{Wilson1972-theoretical}.
Much theoretical work has been done since this cautionary tale. Yet the
same problems of being blinded by new information (to the detriment of
deductive thinking) face modellers now, probably to a greater extent.
This, in combination with the fame enjoyed by early theoretical geographers
(as opposed by more recent empirical geographers who modified or
rejected their work) goes a long way to explain why
researchers continue to cast back to
the pre-computer age for theoretical
insight. Two of the early theories that are most pertinent to
simulation of travel patterns are Von Th\"{u}nen's work on the
spatial distribution of
agricultural activity and Christaller's central place theory.

Von Th\"{u}nen's work in the early 1800s is a seminal example of this early
theoretical thinking. His model of concentric zones of agriculture was
described verbally and in the evolving language of mathematics but rarely tested
on real data \citep{moore1895thunen}.\footnote{For example, ``although [Von
Th\"{u}nen] claims that his advantage over Ricardo consists in his ability to
reduce the co-operation of capital to terms of labour, the validity of that
claim has not been tested. \citep[p.~126]{Moore1895-thesis}} It exerts a strong
influence, even in the 21$^{st}$ century (e.g.~\citealp{lankoski2008bioenergy}).
Its use of geographically variable variables, strict and clearly
defined assumption and extensibility make it a precursor to modern agent-based
models \citep{sasaki2003agent}. The approach describes individual units based in
living in Cartesian space, that can be seen both as discrete zones, or as
a continuous variable (as an input into the cost of travel)
\citep{Stevens1968a}. The model's insight into the variability of
individual-level behaviour depending on their zone of habitation can therefore
be seen as a direct precursor to spatial microsimulation models. These also
seek to describe the characteristics and behaviour of individual units living in
geographical zones.
%%%!!! Please god give me time to make
% A general version of the Von Thunen model and place it on github -> rinseage
% Rinse this section; concise; go back if needed!!!

Walther Christaller's central place theory of the 1930's provided an integrated
theory of spatially variable behaviour (primarily shopping) and the location of
settlements of varying sizes \citep{matthews2008geography}. Based on the
assumption of a continuous and even geographical space ready for urban growth,
the theory proved fertile for hypothesis testing and extension based on
increasingly complex mathematics . Following Von Th\"{u}nen, Christaller
attempted a `scientific' explanation of the behaviour of individuals based on
where they live. The mechanistic nature of the approach has since been
superseded by more advanced and probabilistic models
yet central place theory continues to influence
many areas of spatial modelling \citep{Wilson1972-theoretical, Sonis2005,
Farooq2012-integreted}. Applied to commuting, the theory provides a ready-made
model about where people travel to work: the settlement that can provide the
best pay, minus travel costs. Of course, both pay and travel costs vary greatly
depending on a number of individual and geographic variables that cannot be know
in every case. However estimates can be made (even in the absence of now
readily available data) and applied stochastically in mathematical
formulae. This theoretical approach has subsequently helped
explain spatial distributions in travel to work patterns, using models based on
Christaller's ideas \citep{Tabuchi2006-commuting-costs}. Indeed, Christaller was
a major advocate of explaining theories in
mathematics: ``the equilibrium of the location system ... can only be
represented by a system of equations'' (Christaller, 1933; quoted in
\citealp[p.~35]{Wilson1972-theoretical}). Whether or not this statement is true,
and its potential applicability to commuting patterns, is discussed in
\cref{Chapter8}. More prosaically, Christaller's theory also helps explain why
long-distance commuting is far more common into large cities than small ones
\cref{Chapter6}.
% !!! REallly? + maybe more precisely located cross ref.

The preceding discussion provides only a small snapshot of pre-computational
spatial analysis, based on two influential thinkers. The
focus was on deductive reasoning, rather than inductive methods, whereby large
amounts of data are processed in the hope of finding some underlying pattern.
This emphasis can provide a lesson for the future: despite the clear
disadvantages faced by researchers before the digital revolution, one
advantage they seem to have had was a clear theoretical focus, and this may
have been due in part to absence of large and distracting datasets and
computers.
The danger that this historical perspective flags is that the masses of
micro-level data now available could distract from explanation. As
\citet{Wilson1972-theoretical} emphasised, it is explanation and theory
development, not mere description, that enables a discipline to progress. 

Despite this risk, the emergence of powerful computers have allowed theories to
be developed and tested in ways that were previously impossible. The digital
revolution can thus be seen as the single most important event in the history
of spatial simulation.

\subsection{The digital revolution}
\label{s:digirev}
\begin{quote}
 At the present time, the speed and capacity
of electronic computers would still put economic limits on the number of units
that could be handled in the above fashion.
\flushright{\citep[p.~120]{Orcutt1957-new-type}}
\end{quote}

After World War II a number of factors drove interest in modelling human
behaviour and transport. Important among these were a couple of influential new
technologies: the mass produced car and electronic computers. The former
expanded rapidly in the West before the oil price shocks of the
1970s, during a sustained period of stability and economic growth. Nowhere
was this more apparent than in the USA, where the rapid uptake of the car was
forcing planners to reconsider city layouts in order to cope with the influx.
Linked to this pressure, the broadly defined art/science of `Urban Modelling'
also
began, originating in the USA \citep{batty1976urban}. Planning for the future
of cities in a resource-constrained world was a research priority for some,
even before the severity of environmental problems such as climate change was
fully understood \citep{Rouse1975}. The potential of numerical models to solve
this problem was
not overlooked, although much attention was allocated towards accommodating
forecast shifts 
towards the car rather than to consider relative performance of
radically different options at the outset \citep{manheim1968search, TUI1972}.

Beyond changing mobility patters (the impact of which was largely to provide
motivation, but not method), it was the appearance of computers that drove
forward and facilitated progress in the field. Although many now
take fast and efficient processors for granted (e.g.~using hand-held computers
to play `Angry Birds' and check Facebook accounts), they affected vital areas of
daily life, from
education to the design of traffic lights. The digital revolution should not
be seen as a single transformative event: it is an ongoing and accelerating
set of changes in the way we interact with, process and communicate
information. Combined with the internet, the digital revolution has ongoing
impacts on society \citep{Rushkoff2011}, including travel to work patterns
\citep{Orloff2003} and of course the methods available to investigate human
behaviour over space.

\emph{Computing power}

As with other areas of rapid technological progress, there is no
fixed point at which one can say we have `enough' computing power to solve the
most pressing transport issues: An interesting phenomenon with computing power
is that, much like the problem of roads driving demand for driving up, the more
there is the more demand grows. Throughout the 20$^{th}$ century computing power
was often seen
as \emph{the} limiting factor preventing accurate simulation of social systems.
This is well illustrated by the quote that begins this
section. To
put the quote into its proper context, consider the following: the
IBM 704 had the equivalent of 18,432 bytes of RAM. This was the first mass
produced computer and was
considered as the state of the art at the time of Orcutt's paper:
subsequently in the article it was referred to as a `powerful giant' 
\citep{Orcutt1957-new-type}. Now one can purchase a laptop with 16 Gigabytes of
RAM for approximately 5\% of average UK wages (\pounds1,000). This is
1,000,000,000
times more memory than was available to the IBM, operating millions of times
faster and costing thousands of times less in real terms. Yet still people
complain about lack of computing power! In other words, as computing power has
advanced exponentially, approximately by Moore's law --- which accurately
predicts the exponential shrinkage of electronic components, by a factor of 0.7
every 3 years \citep{kish2002end} --- our hunger for more and faster processing
has increased even faster.

Regardless of our insatiable thirst for processing power, these external factors
--- the digital revolution and wider societal changes, embodied in the the
car --- undoubtedly drove forward research seeking to understand and model
transport systems in detail. The aim was to harness the marvel of computing
power to better understand the rapid shifts taking place.
This was most apparent in applied urban modelling: ``Increasing car
ownership during the 1940s and early 1950s led to the growing realisation
that cities with their traditional physical form could simply not cope
with the new mobility'' \citep[p.~6]{batty1976urban}. The new methods formed an
important tool for enabling planners to deal with this shift. %%% equations!
% after 3.2.3 drafted!

In statistics too, more
sophisticated methods were being considered. Increasingly large and complex
datasets were an additional driver of advancement here: the increased automation
and rigour of data collection led to new data management
problems. Placing his seminal work on iterative proportional fitting (IPF) in
context, \citet[p.~427]{Deming1940} provides the following example of this
data-driven methodological development:
``in the 1940 census of population a problem of adjustment arises from the fact
that although there will be a complete count of certain characters for the
individuals in the population, considerations of efficiency will limit to a
sample many of the cross-tabulations (joint distributions) of these
characters.'' More than 70 years later, his methods are still in use, to
tackle the same issue, as well as a host of others \citep{Jirousek1995}.

Deming's methodological innovation was not especially outstanding in the
context of rapidly advancing 1940s statistics. But it is worth considering in
more detail here: the iterative proportional fitting procedure, that built on
\citep{Deming1940} has become a frequently used in spatial microsimulation
models, for the task of automatically allocating individuals from a survey
dataset to the zones for which they were most representative.
New applications and refinements to Deming's method continued in the
proceeding years within statistics
\citep{stephan1942iterative,Friedlander1961-ipf}, although the term `iterative
proportional fitting' (IPF) was only used to describe it after
\citet{Fienberg1970}. Since then, IPF has continued to be refined and applied to
various statistical problems involving the estimation of missing data, but
these advances are generally contained in a literature that is separate from
the body of work that is the focus of this
chapter.\footnote{As
a relevant
aside, history of IPF provides an interesting example of fragmentation in
academic research, as the statistical community continued to use Deming and
Stephen's method of estimating internal cell values based on known marginal
subtotals,
but using a totally different name: ``The methodology became known as `raking'
and found widespread application in sampling, especially at the US Census Bureau
and other national statistical offices''  \citet{Fienberg2007}. It is important
to note this divergence, as the statistical uses of IPF (or `raking') have the
potential to aid the technique's usage in spatial microsimulation.
}

It was only with the intervention of Guy Orcutt that such methodological
advancements were combined with new computing capabilities to provide new
possibilities for social science, based on the simulation of individuals.
Although Orcutt is often cited as one of the founders of social simulation,
arguably his most important contribution was to place computerised methods in a
wider conceptual framework of policy analysis. Instead of using a
single `representative agent' with averaged values, the microsimulation method
enabled the evolution of multiple micro units to be traced, under different
scenarios \citep[p.~176]{mitton2000microsimulation}.
This helps explains why Orcutt(\citeyear{Orcutt1957-new-type,
orcutt1961microanalysis}) is frequently cited as one of the founding fathers of
the field
(e.g.~\citealp{Clarke+Longley1989-UK-housing-sim,Wu2008,
Ballas2013-4policy-analysis}). Granted, he successfully exported the concept of
manipulating individual-level variables based on estimated
probabilities of change, but Orcutt was not particularly interested in
spatial analysis.\footnote{Although
Orcutt was instrumental in advocating and demonstrating
micro-level methods for policy evaluation, he was more concerned with time than
he was with
space. %%%!!!referred
Neither IPF nor combinational optimisation, two of the main tools used for
generating spatial microdata in spatial microsimulation research today,
%!!!verify
are mentioned in his seminal works
\citep{Orcutt1957-new-type,orcutt1961microanalysis}.
Instead, he laid down the tantalizing possibilities of simulating society, in
very general (and seldom validated) terms, using the newly available
mainframe computers. The following is a typical example of the clarity,
enthusiasm and sense of purpose of his vision: ``The following method is
feasible, readily comprehensible, and may serve to illustrate still further the
proposed model. Using this approach the model would be simulated on a large
electronic machine, such as the IBM 704 or the UNIVAC II, or some improved
successor to these powerful giants'' \citep[p.~119]{Orcutt1957-new-type}.
}
Building on Orcutt's methods, the technique of simulation grew popular in the
increasingly quantitative social sciences. Uptake was
greatest in economics, where the technique
gained a strong following as a technique for evaluating the impact of
changing policy and economic conditions at the individual level
(see \citealp{Merz1994} for an overview).
The branch of microsimulation associated with spatial problems emerged later
\citep{Tanton2013-intro}, although it has clear links with earlier shifts
towards modelling within the wider field of quantitative geography
(e.g.~\citealp{Clarke1985}).

The shift to the practical application of microsimulation to explicitly
\emph{spatial} problems was not to happen until around 30 years after the
1960's applications. This can partly be attributed to the
computational limits emphasised by Guy Orcutt at the outset of this Chapter, but
partly also to a disinterest in quantitative models on the part of geographers.
A seminal paper \citep{Holm1987} reviewed the limited experience of
microsimulation models for
spatial applications up to that point. The authors warned of ``the
possibility of the method being reinvented by different
researchers independently'' if the new techniques continued to be ignored by
geographers \citep[p.~145]{Holm1987} and provided a coherent argument in favour
spatial microsimulation, culminating in the following conclusion:
``With micro-modelling it is possible to use and formulate theoretical concepts
and hypotheses about social action on at least the same level of detail as
sometimes found in other social sciences  without neglecting the apparent and
important elements of spatial interdependence seldom found in studies outside
geography'' \citep[p.~163]{Holm1987}.
Thus the gauntlet was laid down to future
researchers entering this emerging field: develop spatial microsimulation models
to take advantage of newly available computers, programming languages and
datasets. Since then ``the speed of development has gathered
pace''\citep[p.~259]{clarke2013conclusions}. Spatial microsimulation is now a
field of social and spatial analysis in its own right, with an expanding range
of applications. Transport, along with a number of other phenomena, has been
identified as an area for future application of the modelling framework
\citep[p.~270]{clarke2013conclusions}.

\subsection{Modern spatial microsimulation}

Geographers are not generally taught computer programming.
This, and the `erosion of quantitative literacy' \citep{ESRC2013}
helps explain why spatial microsimulation has been limited to a
small field within geography and related disciplines. Spatial
microsimulation now constitutes ``a relatively small
community'' that can be considered a field in its own right
(Wilson, in \citealp[p.~vi]{Tanton2013}).

This community can roughly be identified as those with links
to the International Microsimulation Association (IMA), 
who publish spatial microsimulation work in peer reviewed
journals\footnote{The following journals are common places for the
publication of spatial microsimulation research:
\emph{Computers, Environment and Urban Systems},
\emph{The international Journal of Microsimulation},
\emph{Journal of Artificial Societies and Social Simulation} and
\emph{Environment and Planning A}. Applied spatial microsimulation
research is also published in a wide range of regional science
and geography journals.
}
and whose work is referred to in recent overviews of the field
\citep{Tanton2013, O'Donoghue2013}.
In summary, spatial microsimulation has emerged
from pre-computer origins and mid 20$^{th}$ century theoretical quantitative
geography to the tackle the research challenge set out by
\citet{Holm1987}. Since powerful computers became available at the turn
of the 21$^{st}$ century, methods and applications have
proliferated and accelerated. Spatial microsimulation now
provides small-area estimates
of individual-level variables and projections of future change
for various applications, yet there is much scope for
future development, including application to transport issues
\citep{Clarke2013-concs}.

% Microsimulation models clearly had great potential yet many of the researchers who
% could benefit most from them lacked the skills to `get stuck in' and build
% spatial microsimulation models from scratch. Even if scripts were made available, it
% would have been difficult to find suitable computers and programming expertise to
% run them until the turn of the century, by which time approaches to spatial microsimulation
% were moving on. Spatial microsimulation is undoubtedly a fast moving field, in which
% a number of key papers have had a large effect on subsequent studies. To gain an understanding
% of the reasons behind the current state of the art \cref{s:sotart}, seminal papers that
% have helped to consolidate and define the field are presented below:
% \begin{itemize}
%  \item \citet{Holm1987}
%  \item \citet{Williamson1998}
%  \item \citet{}
% \end{itemize}
% 
% \citep{Birkin1989}
% \citep{ballas2003microsimulation-30-years}
% % Tell a story: it started in Africa, uptake by geographers
% \citep{Wong1992}
% \citep{Johnston1993}
% 
% The first national dynamic spatial microsimulation model was the System for
% Visualising Economic and Regional Influences in Governing the
% Environment (SVERIGE) \citep{vencatasawmy1999building}. This model, unlike the
% static approach used in this PhD, is fully dynamic and includes modules which
% calculate fertility, mortality, employment and other life events including
% migration. SVERIGE focuses on agent life events, rather than the geographical
% distribution of their individual-level attributes and how they were allocated to
% different
% places.\footnote{In fact, SVERIGE did not need to allocate any individuals from
% an a-spatial dataset to zones based on linking variables. This is because a
% micro-level dataset containing all Swedish individuals is available to
% researchers \citep{Ballas2005-ireland}. This removes the need for
% spatial microsimulation via reweighting as defined above. 
% 
% } The dynamic nature of 
% SVERIGE and its basis in real microdata make it a powerful tool for evaluating
% the impacts of national policy changes \citep{ Ballas2013-4policy-analysis}
% 
% an early spatial agent-based model (see
% \cref{s:agent-based}), rather than as an early example of dynamic spatial
% microsimulation as described by .

\section{Spatial microsimulation: state of the art}
\label{s:sotart}
Spatial microsimulation can now be seen as a field in its own right, with roots
in Economics, Geography, Statistics and Regional Science.  It is rapidly
evolving, so any rigid definition of the `state of the art' is likely to become
obsolete quickly. Instead, the scope of spatial
microsimulation is explained in terms of the types and applications of models
in use, the variety of reweighting algorithms and recent transport applications.

\subsection{Types of spatial microsimulation models} %%% Add applications 
\label{types-msim}
%%% Taken directly from ints paper
The wide range of methods available for spatial microsimulation can be divided
into static, dynamic, deterministic and probabilistic approaches (Table
\ref{typology}). Static approaches generate small
area microdata for one point in time. These can be classified as
either probabilistic methods which use a random number generator, and
deterministic reweighting methods, which do not. The latter produce
fractional weights. Dynamic approaches project small
area microdata into the future. They typically involve modelling of
life events such as births, deaths and migration on the basis of random
sampling from known probabilities on such events \citep{Ballas2005c,
Vidyattama2010}; more advanced agent-based techniques, such as spatial
interaction models and household-level phenomena, can be added to this basic
framework \citep{Wu2008, Wu2010}. There
are also `implicitly dynamic' models, which employ a static
approach to reweight an existing microdata set to match
projected change in aggregate-level variables
(e.g.~\citealp{Ballas2005-ireland}).

\begin{table}[h]
\centerline{}
\caption{Typology of spatial microsimulation methods}
\vspace{0.25 cm}
\footnotesize{
\begin{tabular}{p{1.6cm}p{2.4cm}p{3.5cm}p{3.0cm}p{2cm}}
\toprule
{Type} & {Reweighting technique} & {Pros} & {Cons} &
{Example} \\ \midrule
\multirow{3}{2cm}{\vspace{0.3cm} \\ Determ-  inistic\\\vspace{0.3cm}
Re-\\weighting} & Iterative proportional fitting (IPF) & Simple, fast, accurate,
avoids local optima and random numbers & Non-integer weights &
\citep{Tomintz2008}⁠⁠ \\ \cmidrule{2- 5}
& Integerised IPF & Builds on IPF, provides integer
weights & Integerisation reduces model fit & \citep{Ballas2005c}⁠ \\
\cmidrule{2- 5}
&  GREGWT, generalised reweighting
& Fast, accurate,
avoids local optima and random numbers & Non-integer weights  &
\citep{Miranti2010}⁠ \\ \midrule
\multirow{2}{2cm}{ \\ Probab- ilistic  Combin-
atorial optim-
isation} & Hill climbing approach & The simplest solution to a combinatorial
optimisation, integer results & Can get stuck in local optima, slow &
\citep{Williamson1998}⁠ \\ \cmidrule{2-5}
& Simulated annealing & Avoids local minima, widely
used, multi-level constraints & Computationally intensive
& \citep{kavroudakis2012}⁠  \\ \midrule
\multirow{2}{2cm}{\vspace{0.3cm} \\ Dynamic} & Monte Carlo
randomisation to simulate ageing  & Realistic treatement of stochastic
life events such as death & Depends on accurate estimates of life event
probabilities & \citep{Vidyattama2010}⁠ \\
\cmidrule{2- 5}
& Implicitly dynamic & Simplicity, low
computational demands & Crude, must project constraint
variables & \citep{Ballas2005b}⁠ \\ \bottomrule
\end{tabular}
}
\label{typology}
\end{table}

In practice, the typology presented in \cref{typology} is an
oversimplification. The spatial microdata generated during the same spatial
microsimulation project can be used of both static and dynamic applications and
different reweighting algorithms can be applied to the same dataset with
similar results. Spatial microsimulation can thus be seen as an evolving
process rather than a `once-through' analysis. A typical spatial microsimulation
project, for example, may involve some or all of the following four steps
(the first four are from \citealp{ballas2003microsimulation-30-years}):
\begin{itemize}
 \item Construction a micro-dataset, often from surveys.
 \item Reweight the individual-level data to create a spatial microdataset.
 \item Static what-if scenarios (implicitly dynamic scenarios in
\cref{typology}) to asses the impact of instantaneous change.
 \item Agent-based modelling, to better understand how the individuals in each
zone interact with the environment and each other.
\end{itemize}

% The applications of spatial microsimulation can also be usefully categorised
% to inform discussion on the method's expanding range of uses.
% 
% \citet{van2012multifunctional} harnessed a spatial microsimulation model
% to provide input data for a logit model of shopping preferences.
% This modelling exercise provided estimates of the
% impact of new a supermarkets on surrounding stores: it was calculated
% how much more or less would be spent in each. This novel use of
% spatial microsimulation for scenario-testing is highly policy-relevant
% (and politicised), and could be made more so by calculating distributional
% impacts.

\subsection{Reweighting algorithms} %%% Could easily put the IPF paper here
To run a spatial microsimulation model, a prerequisite is a mechanism by
which individuals from the survey are selected to `populate' the areas under
investigation. For the technique to be worthwhile, it is vital that individuals
who are in some way representative of each area should be selected
\citep{Ballas2005}⁠. Doing this manually is clearly not feasible, so a number
of computerised techniques have been developed to create weight matrices
automatically. This section provides an overview of the reweighting techniques
that have been used in published research; the findings fit directly into the
choice of microsimulation model used in this research. %(Illustration here???)

Reweighting algorithms allocate individuals counts or weights for target
areas based on a number of matching or linking variables that are shared between
area and survey datasets. A number of options are available, and these can be
broken down into the following categories:
deterministic/randomised, integer/ratio, and count/weight.
The option used in this thesis is deterministic
sampling based on IPF. This reweighting procedure was
chosen due to the repeatability of the
results,\footnote{``One
advantage of a deterministic model is that the estimated population
distributions will be the same each time the model is run'' \citep{Smith2009}⁠.
Thus, the results of any model to be replicated
without the need to ``set the seed'' of a known list of
Pseodo-random numbers \citep{Robert2009}: this makes results easier to
test and update when new data emerges.}
relative simplicity, and past experience with
the technique.

Randomised (combinatorial optimisation) sampling strategies have the advantage
of robustness against local
optima, which may mean that deterministic models may not always arrive at the
optimal solution \citep{Williamson1998}.
Also, a combinatorial optimisation sampling strategy has the inherent advantage of
keeping individuals as integers (as opposed to deterministic reweighting,
which results in fractional weights). This makes it easier to
understand the simulated population, analyse the results 
(e.g.~ the Gini Index calculation
is more straightforward if integer weights are used), and select
subsets of the simulated population with certain characteristics.
In addition, integer weights are needed for agent-based models. On the other
hand, integer results can be associated with large differences between simulated
and actual cell values \citep{Ballas2005}⁠.

In order to calculate the probabilities of survey individuals appearing in
statistical areas, iterative proportional fitting (IPF) has been used . By
altering the cell values in a 2 dimensional matrix, IPF is used to match
``disaggregated data from one source with the aggregated data from another''
\citep[p.~1]{Norman1999a}⁠. This is done iteratively: each iteration brings the column and
row totals of the simulated dataset closer to those of area in question.

Another, more fundamental, disadvantage of IPF is its inability to simulate
individuals based on data at multiple levels, for example household and
individual: ``it can control either for agent-level or for group-level
attributes but not for both simultaneously'' \citep[p.~5]{Muller2010}.
This problem has long challenged researchers because ``working at the
household/family and person levels simultaneously can introduce conflicts
between the competing goals of achieving good fit at both levels''
\citet[p.~694]{Pritchard2012}. \citet{Pritchard2012} have tackled this problem
by matching either individuals to known family attributes or vice versa, and
using  to it based on conditional probabilities of the spouse sharing given
attributes (age, eduction). These results offer the promise of allowing
family-level microdata generation from deterministic reweighting algorithms
such as IPF.

Despite the wide range of reweighting options available and even wider
range of implementations, there has been relatively little work comparing
different approaches. Most model experiments evaluate goodness-of-fit for
only a subset of reweighting algorithms, changing just one or two variables
at a time \citep{Voas2000, Smith2009, Rahman2010}. Another problem is the
wide range of evaluation tools on offer, leading to confusion about
which method is appropriate for a given application:
``Different researchers use different methods to test the
reliability of their results. This makes it more difficult for `outsiders'
to evaluate the value of a model or set of artificial population data''
\citep[p.282]{Hermes2012a}. This issue is tackled with respect
to the problem of integerisation in \cref{s:integerisation}, and
an open-source testing suite and research program
for community evaluation of different
microsimulation tools is outlined in \cref{meval}. One group
of `outsiders' that could benefit from more accessible code and
reproducible testing of it is the transport community, who are increasingly
turning to spatial microsimulation to meet the need to include
social factors in scenario evaluation.

\subsection{Transport applications}
%%% This is the section I need to write next 4 this chap. First I'd like some results.
It was mentioned in \cref{s:defs} that `population synthesis' is a synonym for
(static) spatial microsimulation. The term is used by transport modellers
to describe the
process of generating individuals as inputs into wider transport models.
Thus spatial microsimulation is used in transport applications.
Whether to classify any give transport study as spatial microsimulation for
transport analysis, or a transport model with spatial microsimulation
`bolted on', is a question of semantics not dwelt on
here.\footnote{\citet{Ballas2013-4policy-analysis}
treat all activity-based
transport models as a subset of spatial microsimulation;
the approach taken here is to deal with models with roots in
spatial microsimulation (this section) separately from
dedicated transport models (\cref{s:dedicated}).}
In any case, there is clearly a
large degree of overlap between
the two approaches. This section describes transport research that focuses on the
individual (human, not vehicle) level, primarily through spatial microsimulation.
\Cref{s:dedicated} outlines dedicated transport models, which can also harness
spatial microsimulation data as an addition to assess social impacts.

Transport modelling has a long history that with strong links to engineering
,\footnote{The strength of engineers'
influence is emphasised in the following passage: ``If the main brief
of the planners is to recommend the `shape' of cities,
then it is usually left to the engineers to design, build and manage the
transport systems. Engineers,
therefore, can use models as design tools: for predicting loads ...
network optimisation ... they will have concerns with project
appraisal'' \citep[p.~16]{Wilson1998-past}.}
strategic planing \citep{Wilson1998-past} and hence large contracts.
Aggregate economic return on income has thus played a central role in
project evaluation, and has become a focus of various modelling efforts
\citep{Masser1992}.
Perhaps due to this narrow technical and economic heritage, traffic models
have tended to omit people from the analysis. Technical
questions, such as `how much congestion will intervention x alleviate?',
predominate, rather
than social questions more common in spatial microsimulation research
(e.g.~`which groups will benefit most from intervention x?')
Thus it has been rare for socio-economic variables to be included in
the model-based evaluation of transport projects, although
social impacts are increasingly considered \citep{Masser1992, Tribby2012}.
This explains growing interest
in spatial microsimulation for transport applications.
It is in this context --- a divide between the transport community, with its focus
on traffic and aggregate economic performance, and the spatial microsimulation
community, with its focus on distributional impacts and public policy --- that these
studies are conducted. 


\citet{Pritchard2012} advocated harnessing spatial microsimulation
for methodological reasons, including the computational benefits of sparse data storage
for transport models.\footnote{Sparse storage here refers to data structures
whereby only non-zero values are stored and replication
weights are used instead of repeating statistically identical individuals multiple
times. This also avoids problems associated with arbitrary categories, e.g.~for
age: ``Complete array storage is proportional to the number of categories used
for each attributes, while the sparse storage scheme is not affected by the
categorization of the attributes'' \citep[p.~691]{Pritchard2012}.} These
efficient % could add table 1 from here - cool!!!
data structures have origins in early spatial microsimulation
research \citep{Holm1987, Williamson1998} and have the additional benefit
of providing ready-made inputs into agent-based transport models such as
ILUTE (see \cref{s:dedicated}).


\emph{PopGen} is a program used
to generate spatial micro-data on the characteristics of individuals
living, and using transport services, in the study region
\citep{Ravulaparthy2011}. It \index{PopGen}
is essentially a static spatial microsimulation model that combines non-spatial
survey data with `marginal tables'
(constraint variables). Three input files can be used at each level ---
individual, household, and optional `groupquarters'
(these are generally students living away from home) --- leading to a high level of
detail. The use of iterative proportional updating (IPU) is key to the
ability of PopGen to simultaneously match individual and household level
characteristics, during the process of allocating individuals to household
\citep{ye2009methodology}. PopGen is made freely available to
anyone from Arizona State University, and has been used as a population
synthesizer for other tranpsort studies \citep{pendyala2012application}.

\emph{Popgen-T} is a different (albeit confusingly similar in name) population
synthesiser developed specifically for the purpose of analysing the
distributional impacts of new transport schemes such as congestion charges
\citep{Bonsall2005}. The method uses IPF to combine data from a very wide range
of sources, although the exact mechanism is not
explained.\footnote{In the 2005 paper, the following information on
data sources was provided: ``The data sources used in this application include the
Household Census, the National Travel Survey, the Journey to Work Census,
the Household Income Survey, The Household Expenditure Survey,
the New Earnings Survey and a number of local travel surveys'' \citep[p.~410]{Bonsall2005}.
The data are further explained in a 2002 working paper, but this could not
be found.} Since the 2005 paper, no further implementations of the Popgen-T method
could be found.




\section{Microsimulation in urban modelling}
\label{s:urbanmodel}
Urban modelling goes beyond the estimation of individual-level
characteristics, as performed in spatial microsimulation.
It attempts to include influential factors from the entirety of
urban experience, from house prices and the labour market to
the transport network and land-use. It is therefore inherently
an ambitious project, that could claim to encapsulate transport models
and explain travel to work pattern in their wider context.
Only recently have data and computational power emerged to
make this `dream' reality; many of the approaches to urban
modelling are related to this PhD. The most relevant are outlined below.

Five entities central to any urban model have been identified
by Wilson (2000) \citep{ballas2003microsimulation-30-years}, and
it is the interaction between these that determines
the final model outcome. The importance of each
for influencing commuter flows, level of data availability and
ease of incorporation into quantitative models is presented in
\cref{t:entities}. Ultimately, these considerations determined
whether, and at what stage, each of these entities were included.
\begin{table}[htbp]
\caption{Five entities central to urban modelling, after Wilson (2000)}
\begin{tabular}{p{2cm}p{3.5cm}p{3.5cm}p{3.5cm}} \toprule
Entity & Data availability & Importance for commuter flows & Ease of model inclusion \\ \midrule
People & High: commuting data collected in the Census and surveys & High: personal behaviour & High: individuals are basic unit of analysis \\
Organisations & Low: rapid change (especially in private sector operators) and poor accountability in many cases & Medium: councils and companies influence travel patterns & Low: organisations often
diffuse bodies \\
Commodities, goods, services & Low: petrol sales and bus ticket data not publicly available & Medium: travel is affected by price of fuel & Medium: can be defined by price of oil; depends on
elasticity \\
Land & High: maps of terrain and land use readily available & Medium: network distance and terrain affect travel behaviour & Medium: via influence of topology and distance \\
Infrastructure & Medium: Open Street Map and Ordnance Survey data & High: personal travel depends on infrastructure & Medium: can affect local travel decisions \\
\bottomrule
\end{tabular}
\label{t:entities}
\end{table}
Based on the basic multi-criteria analysis presented in
\cref{t:entities}, the following
hierarchy of entities for inclusion was established, in descending order
order of priority:

$people > infrastructure > land > commodities > organisations$.

The available data are therefore presented in this order, after a
brief discussion of an additional data source that is
vital to the research problem: energy use in transport.

\emph{Agent based models}
\label{s:agent-based}
\subsection{Dedicated transport models}
\label{s:dedicated}
Transport modelling is a large field within the wider framework of
urban modelling. It has a long history, but has undergone a rapid
evolution in the last decade, largely due to the emergence of the
internet allowing large collaborative software projects to flourish.
Three dedicated transport models, of increasing levels of sophistication
have been selected from the
vast array of options (see \citealp{Rasouli2012} for a technical review).

\emph{SATURN} is a commercial transport model, originally
developed at the University of
Leeds \citep{boxill2000evaluation}. Its current incarnation
is version 11, a stable package running only on Windows \citep{SATURN2012}.
SATURN model is a mature tool for determining traffic loads on road networks
networks given a known origin-destination flow matrix,
and is used for this purpose in UK local government \citep{boyce2005urban}.

\emph{OpenTraffic} addresses many of the issues arising from commercial,
closed-source traffic simulation models such as SATURN: ``Most commercial
traffic simulation packages primarily offer only ready-to-use
functionality and do not facilitate the addition of new
functionality by users or provide a trans- parent picture of how the
underlying components are implemented'' \citep[44]{Tamminga2012}.
This recently developed simulation framework has a modular design,
and is therefore useful in a wide range of applications, from
`car follow' to activity planning \citep{Tamminga2012}.

\emph{MATsim} is a more mature open source transport model that improves
on previous transport modelling programmes in a number of ways
\citep{rieser2007agent}.
The model allows individual attributes to be maintained throughout
agent-based simulation and ensures that trips made throughout the
day are realistically inter-dependent (e.g.~being late for one trip
will have an impact on the start-time of the next).
Since the project was first made available as a free open source project
in 2006 (see \href{http://sourceforge.net/projects/matsim/files/MATSim/}{sourceforge.net}),
uptake has been rapid with applications ranging from agent-based
modelling of trips for leisure and shopping \citep{horni2009location}
to intensive
performance testing, in which MATSim's is shown to accurately
model real world travel patterns \citep{balmer2008agent, gao2010comparison}.
MATSim has also been used to model commuter patterns in Pretoria, South Africa,
incorporating previously omitted trip-chaining behaviours \citep{van2011agent}.


\subsection{Land-use transport models}
% ``The first generation urban models were designed and implemented in
% North America mainly during the years 1959-68, years which coincided
% with the launching of large-scale land-use-transportation studies in major
% metropolitan areas.''
Researchers now have decades of experience modelling individual agents,
transport flows and the land-uses that lead to personal transit to take place.
Of course, each of these elements depends to some extent on the others, so
integrated land-use transport models have long been regarded as the holy grail
in urban modelling. It is only recently, however, that the vast computational
requirements of this task have been
available.\footnote{The memory requirements alone of storing a detailed
transport network in RAM are large. Combining this with complex polygons
defining administrative zones, a detailed microdataset and then performing
calculations defining how each model object changes from one moment to the next
in high temporal resolution is clearly a taxing computational task.
}
Despite the daunting complexity and data and computational requirements of such
models, their design and implementation has often been dreamt of
\citep{timmermans2003saga}. Indeed, it is only with modern computers and software
that integrated land-use transport models have become a reality: ``recently,
the development of large-scale integrated land-use and
transportation microsimualtion systems such as ILUTE ... ILUMASS
... and UrbanSim has generated a new excitement in the field'' \citep[p.~935]{Pinjari2011}.
These, and TRANSIMS are outlined below.

% A number of integrated models have been developed
% \emph{RAMBLAS}

\emph{ILUTE} \index{ILUTE} represents the `third wave' of transport-land use
models based on individual-level data:
``[it] represents an experiment in the development of a
fully microsimulation modelling
framework for the comprehensive, integrated modelling of urban transportation-land use
interactions, and, among other outputs, the environmental impacts of these interactions''
\citep[p.~15]{timmermans2003saga}. Thus ILUTE can can be used to analyse a wide range
of phenomena: it is an integrated urban model in the fullest sense of the word
and has been even been used to analyse the distribution of house prices
in and large city over time \citep{Farooq2012-integreted}.

%\emph{TRESIS}
\emph{UrbanSim}, like ILUTE, is a micro-level integrated land-use transport
model, aimed at \index{UrbanSim}
``incorporating the interactions between land use, transportation, the economy,
and the environment'' (\href{http://www.urbansim.org/Main/WebHome}{urbansim.org}, 2012).
The source code (written in Java and Python)
is open source and remains under continued development \citep{Waddell2003}.
Perhaps because the software is free for anyone to download, use and modify,
it has been used for range of applications including as a tool to
to aid planners in the evaluation of transport projects \citep{Borning2008}.
Although UrbanSim does not contain an advanced transport module,
work has been done to integrate the dedicated transport MATSim model (see
\cref{s:dedicated}) into it,
via a plug-in \citep{Nicolai2012-matsim}.

\emph{TRANSIMS} \index{TRANSIMS} was developed at the Los Alamos National
Laboratory with an
ambitious objective mirroring that of ILUTE:
``to model all aspects of human behaviour related to
transport in one consistent simulation framework''
\citep[p.~1]{nagel1999transims}.  The model, which is based on cellular
automata, has been given a public licence (the NASA Open Source Agreement
Version 1.3), is cross-platform (with Windows and Linux binaries) and has been
widely adopted as a result.\footnote{``TRANSIMS'' was cited 166 time in Google
Scholar in 2012 publications, many of which implemented the model for their own
applications.}
% It would be good to have ``n. times cited, 2012'' as a variable in comp. tab.
The encouragement of community contributions and an experienced development team
has led the model to be extended various ways. For example, TRANSIMS can be
configured to take advantage of parallel processing (in which one CPU was
allocated to each area being modelled) \citep{nagel2001parallel}, or external
programs for the visualisation of results
(http://sourceforge.net/projects/transimsstudio). The sub-modules of TRANSIMS
include a micro-level population synthesizer, a trip generator, route planner
and microsimulator (which determines the location and behaviour of each
individual at each time step). The model is being increasingly adopted by
Municipal Planning Organizations (MPOs) in the USA \citep{lawe2009transims,
ullah2011travel}, and has successfully simulated the entirety of Swiss travel
flows (around 10 million trips), using a `Beowulf cluster' of parallel computers
\citep{Raney2003}.

The modular design of TRANSIMS means it can be used in conjunction with the
spatial microsimulation methods presented in this paper. The small area
microdata could, when allocated home-work pairs, be used as an input forming
the baseline situation at time 0. The potential for combining the spatial
microsimulation methods presented in this thesis with additional modelling
tools is described in chapter 8.



\section{New research directions}
Over time the uses of spatial microsimulation have expanded from a way of
providing quantitative geographers with individual-level data, into a more
general modelling strategy harnessed by many fields. As in many fields, the
rate of change has also increased, due to increased availability of
sophisticated software, large datasets and powerful
computers. One could make the argument that the
uses of spatial microsimulation, as defined above, have become more specialised
as it is adopted by various fields for their own purposes, sometimes under
different names. This fragmentation is aggravated by the fact that
many do not make the code used for their analysis widely available, a
disease prevalent across the sciences \citep{Ince2012}.
% \citep{Clarke2013-concs}

With the benefit of more than 30 years of hindsight, it is possible to reflect
on what works and what does not work so well in spatial microsimulation
research. Summarising a large body of experience,
\citet[p.~197]{Holm2013-design-principles} created the following `wish list' of
factors that future spatial microsimulation researchers should consider
when creating new, or updating existing, models:

\begin{itemize}
\item  Use the most modern software
\item  Use standard methods, shared by many users
\item  Backward compatibility (so keeping our old models and subsystems running)
\item  Avoid relearning
\item  Develop solutions that are theoretically well designed
\item  Transfer knowledge and know-how to new colleagues
\end{itemize}

Effort has been invested throughout the PhD to comply with these
principles. %!!! include these!!!
\label{s:bigdata-gps}
% \subsection{`Big data'}
% \subsection{GPS-loan surveys}
% \subsection{Applications for model validation and enhancement}
\label{s:applications}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Deshets
% In economics, spatial microsimulation has been used to \citep{Cullinan2011}
